\documentclass[10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\decimalpoint
\usepackage{amsmath, amssymb}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{letterpaper, margin=1in}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{tocloft}

\usepackage[colorlinks=true, linkcolor=black, urlcolor=black, citecolor=black]{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT METADATA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Universidad Panamericana \\ 
    Maestría en Ciencia de Datos \\ 
    Datos Masivos \\ 
    \vspace{0.5cm} 
    Proyecto Final: \textit{Pipeline Distribuido de Predicción para Iowa Liquor Sales en GCP}}
\author{Enrique Ulises Báez Gómez Tagle, Luis Alejandro Guillén Alvarez}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. DATASET UTILIZADO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset utilizado}
\subsection{Fuente y descripción}

El dataset utilizado proviene de \textbf{BigQuery Public Data} y contiene registros de ventas de licores en el estado de Iowa, Estados Unidos. Este conjunto de datos es mantenido por el Iowa Department of Commerce y está disponible públicamente para análisis.

\begin{itemize}
  \item \textbf{Fuente:} BigQuery Public Data - \texttt{bigquery-public-data.iowa\_liquor\_sales.sales}
  \item \textbf{Tamaño:} 32,816,143 registros
  \item \textbf{Periodo:} 2012-01-03 a 2025-10-31 (13.8 años)
  \item \textbf{Características principales:}
  \begin{itemize}
    \item \texttt{date}: Fecha de la transacción
    \item \texttt{store\_number}: Identificador de la tienda
    \item \texttt{city}: Ciudad donde se realizó la venta
    \item \texttt{category}: Categoría del producto
    \item \texttt{item\_number}: Identificador del producto
    \item \texttt{sale\_dollars}: Monto de la venta (variable objetivo)
    \item \texttt{bottles\_sold}: Cantidad de botellas vendidas
    \item \texttt{volume\_sold\_liters}: Volumen vendido en litros
  \end{itemize}
\end{itemize}

\subsection{Cardinalidades y dimensiones}

El dataset presenta alta cardinalidad en múltiples dimensiones, lo que lo hace ideal para procesamiento distribuido:

\begin{table}[H]
  \centering
  \caption{Cardinalidades del dataset Iowa Liquor Sales.}
  \label{tab:cardinalities}
  \scriptsize
  \begin{tabular}{@{}lr@{}}
    \toprule
    Dimensión & Valores Únicos \\
    \midrule
    Tiendas & 3,337 \\
    Ciudades & 504 \\
    Productos & 15,183 \\
    Categorías & 185 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Calidad de los datos}

El análisis exploratorio reveló una excelente calidad de datos con mínimos valores faltantes:

\begin{table}[H]
  \centering
  \caption{Valores nulos por campo.}
  \label{tab:missing-values}
  \scriptsize
  \begin{tabular}{@{}lrr@{}}
    \toprule
    Campo & Valores Nulos & Porcentaje \\
    \midrule
    sale\_dollars & 10 & 0.00003\% \\
    category & 16,974 & 0.052\% \\
    city & 84,575 & 0.258\% \\
    \midrule
    \textbf{Total} & \textbf{101,559} & \textbf{0.31\%} \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Calidad general:} 99.69\% de datos completos, lo que indica un dataset de alta calidad para modelado predictivo.

\subsection{Distribución de ventas}

La distribución de la variable objetivo (\texttt{sale\_dollars}) muestra las siguientes características:

\begin{table}[H]
  \centering
  \caption{Distribución de ventas en dólares.}
  \label{tab:sales-distribution}
  \scriptsize
  \begin{tabular}{@{}lr@{}}
    \toprule
    Percentil & Valor (USD) \\
    \midrule
    P50 (Mediana) & \$78.66 \\
    P90 & \$269.88 \\
    P99 & \$1,185.60 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Top 10 categorías por volumen de ventas}

Las categorías más vendidas representan una parte significativa del volumen total de transacciones:

\begin{table}[H]
  \centering
  \caption{Top 10 categorías por ventas totales.}
  \label{tab:top-categories}
  \scriptsize
  \begin{tabular}{@{}rlrr@{}}
    \toprule
    Rank & Categoría & Ventas Totales (USD) & Transacciones \\
    \midrule
    1 & 1012100.0 & \$495,078,200 & 2,778,490 \\
    2 & 1031100.0 & \$441,329,100 & 2,988,622 \\
    3 & 1011200.0 & \$288,427,900 & 1,859,256 \\
    4 & 1081600.0 & \$219,643,200 & 1,360,017 \\
    5 & 1062400.0 & \$169,326,700 & 861,360 \\
    6 & 1022200.0 & \$152,794,300 & 668,286 \\
    7 & 1031080.0 & \$145,760,500 & 1,265,930 \\
    8 & 1022100.0 & \$143,383,100 & 849,580 \\
    9 & 1011400.0 & \$119,534,300 & 538,956 \\
    10 & 1011100.0 & \$117,536,600 & 1,213,606 \\
    \midrule
    \textbf{Total Top 10} & & \textbf{\$2,292,813,900} & \textbf{15,384,103} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Justificación de selección}

Este dataset fue seleccionado por las siguientes razones:

\begin{enumerate}
  \item \textbf{Volumen masivo:} Con más de 32 millones de registros, cumple ampliamente con el requisito de $\geq$32M registros y justifica el uso de procesamiento distribuido con PySpark en Dataproc.
  
  \item \textbf{Datos temporales:} El rango de 13.8 años permite análisis de series temporales y patrones estacionales, ideal para feature engineering temporal.
  
  \item \textbf{Alta dimensionalidad:} La combinación de 15K+ productos, 185 categorías, 3.3K tiendas y 504 ciudades proporciona un espacio de características rico para modelado predictivo.
  
  \item \textbf{Calidad excepcional:} Con 99.69\% de datos completos, minimiza la necesidad de imputación compleja y permite enfocarse en transformaciones y modelado.
  
  \item \textbf{Variable objetivo continua:} \texttt{sale\_dollars} es una variable continua ideal para regresión lineal, permitiendo predecir montos de venta basados en características de productos, ubicación y temporalidad.
  
  \item \textbf{Disponibilidad pública:} Al estar en BigQuery Public Data, facilita la reproducibilidad del proyecto y el acceso sin restricciones de licenciamiento.
  
  \item \textbf{Relevancia práctica:} Los modelos predictivos de ventas tienen aplicaciones directas en optimización de inventario, planificación de demanda y estrategias de pricing.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. ARQUITECTURA IMPLEMENTADA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Descripción de la arquitectura implementada}

\subsection{Diagrama de arquitectura}

La arquitectura implementada sigue un patrón de medallion con dos capas (Bronze y Gold) sobre Google Cloud Platform, integrando servicios de almacenamiento, procesamiento distribuido y análisis de datos masivos.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../figures/architecture/iowa-liquor-sales-ml-pipeline-architecture.png}
    \caption{Arquitectura del pipeline distribuido: BigQuery → Cloud Run → GCS Bronze → Dataproc → GCS Gold → ML Model (***PLANNED***).}
    \label{fig:architecture}
\end{figure}

\subsection{Flujo de datos}

El pipeline implementa un flujo de datos end-to-end con las siguientes etapas:

\begin{enumerate}
  \item \textbf{Fuente de datos (BigQuery):} El dataset público \texttt{iowa\_liquor\_sales} (32M+ registros) sirve como origen de datos. Los scripts de EDA (\texttt{eda\_iowa.py} y \texttt{eda\_iowa.ipynb}) realizan análisis exploratorio inicial directamente sobre BigQuery.
  
  \item \textbf{Extracción (Cloud Run):} Un servicio ETL desplegado en Cloud Run ejecuta \texttt{bronze\_extract.py}, que extrae datos desde BigQuery y los carga en formato Parquet particionado hacia la capa Bronze en Google Cloud Storage.
  
  \item \textbf{Capa Bronze (GCS):} Almacenamiento de datos crudos en formato Parquet con particionamiento temporal, preservando la estructura original para trazabilidad y reproducibilidad.
  
  \item \textbf{Transformación (Dataproc):} Un cluster de Dataproc ejecuta \texttt{gold\_transform.py} con PySpark, aplicando limpieza, transformaciones y feature engineering sobre los datos Bronze. El procesamiento distribuido permite manejar el volumen masivo de forma eficiente.
  
  \item \textbf{Capa Gold (GCS):} Datos limpios, transformados y enriquecidos con features derivadas, almacenados en formato Parquet particionado y optimizados para consumo analítico y modelado ML.
  
  \item \textbf{Modelado ML (***PLANNED***):} Modelo de regresión lineal con PySpark MLlib entrenado sobre la capa Gold para predicción de ventas, con evaluación de métricas (R², RMSE, MAE) y comparación de performance entre configuraciones de cluster.
\end{enumerate}

\subsection{Componentes de la arquitectura}

\begin{itemize}
  \item \textbf{BigQuery:} Fuente de datos pública (\texttt{bigquery-public-data.iowa\_liquor\_sales.sales})
  \item \textbf{Cloud Run:} Servicio ETL serverless para extracción batch hacia capa Bronze
  \item \textbf{GCS Bronze Layer:} Almacenamiento de datos crudos en formato Parquet particionado
  \item \textbf{Dataproc (PySpark):} Cluster de procesamiento distribuido para transformación y feature engineering
  \item \textbf{GCS Gold Layer:} Datos refinados listos para análisis y modelado
  \item \textbf{Terraform:} Infraestructura como código para provisionar clusters Dataproc con diferentes configuraciones
  \item \textbf{ML Model (***PLANNED***):} Modelo de regresión PySpark MLlib para predicción de ventas
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3. DESARROLLO DE LA RUTA ELEGIDA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Desarrollo de la ruta elegida: Procesamiento Distribuido con PySpark}

\subsection{Selección y exportación del dataset hacia GCS}

\subsubsection{Proceso de exportación}

La extracción de datos desde BigQuery hacia Google Cloud Storage se implementó mediante un servicio ETL desplegado en Cloud Run, diseñado para ejecutarse como job batch serverless. El proceso consta de tres etapas principales:

\paragraph{Etapa 1: Creación de tabla temporal con particionamiento.}
El script \texttt{bronze\_extract.py} ejecuta una consulta SQL que selecciona todos los registros del dataset público y agrega columnas derivadas de año y mes para facilitar el particionamiento posterior en PySpark:

\begin{verbatim}
SELECT 
    *,
    EXTRACT(YEAR FROM date) as year,
    EXTRACT(MONTH FROM date) as month
FROM `bigquery-public-data.iowa_liquor_sales.sales`
\end{verbatim}

Esta consulta materializa una tabla temporal en el dataset \texttt{ml\_work.bronze\_temp} del proyecto, permitiendo una exportación eficiente sin modificar la fuente original.

\paragraph{Etapa 2: Exportación a formato Parquet.}
Utilizando la API de BigQuery, se exportan los datos desde la tabla temporal hacia Google Cloud Storage en formato Parquet, un formato columnar optimizado para procesamiento distribuido:

\begin{itemize}
  \item \textbf{Destino:} \texttt{gs://iowa-liquor-medallion-ml/bronze/iowa\_sales/*.parquet}
  \item \textbf{Formato:} Parquet (columnar, comprimido)
  \item \textbf{Particionamiento:} Múltiples archivos generados automáticamente por BigQuery
\end{itemize}

\paragraph{Etapa 3: Limpieza y registro de métricas.}
Una vez completada la exportación, se elimina la tabla temporal y se registran las métricas de tiempo en un archivo JSON almacenado en GCS para trazabilidad.

\subsubsection{Configuración de Cloud Run}

El servicio se despliega mediante un contenedor Docker con las siguientes características:

\begin{itemize}
  \item \textbf{Imagen base:} \texttt{python:3.11-slim}
  \item \textbf{Dependencias:} \texttt{google-cloud-bigquery}, \texttt{google-cloud-storage}, \texttt{pandas}, \texttt{pyarrow}, \texttt{db-dtypes}
  \item \textbf{Tipo de ejecución:} Cloud Run Job (batch, no HTTP)
  \item \textbf{Variables de entorno:}
  \begin{itemize}
    \item \texttt{PROJECT\_ID}: \texttt{secure-cipher-475203-k2}
    \item \texttt{BUCKET\_NAME}: \texttt{iowa-liquor-medallion-ml}
  \end{itemize}
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/cloud-run-images/cloudrun_job_execution_history_config.png}
  \caption{Configuración y historial de ejecución del Cloud Run Job para extracción Bronze.}
  \label{fig:cloudrun-config}
\end{figure}

\subsubsection{Métricas de tiempo de ejecución}

La fase de extracción Bronze completó exitosamente con las siguientes métricas:

\begin{table}[H]
  \centering
  \caption{Tiempos de ejecución de la fase Bronze (Cloud Run).}
  \label{tab:bronze-timing}
  \scriptsize
  \begin{tabular}{@{}lr@{}}
    \toprule
    Etapa & Tiempo \\
    \midrule
    Creación de tabla temporal & 5.51s \\
    Exportación a Parquet (GCS) & 2.81s \\
    Limpieza de recursos & 0.16s \\
    \midrule
    \textbf{Tiempo total} & \textbf{8.51s (0.14 min)} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/cloud-run-images/cloudrun_observability_metrics.png}
  \caption{Métricas de observabilidad del Cloud Run Job mostrando ejecución exitosa.}
  \label{fig:cloudrun-metrics}
\end{figure}

\subsubsection{Verificación de estructura y consistencia}

Una vez completada la exportación, se verificó la estructura del bucket de GCS y la integridad de los datos:

\paragraph{Estructura del bucket.}
El bucket \texttt{iowa-liquor-medallion-ml} queda entonces con la siguiente carpeta:

\begin{itemize}
  \item \texttt{bronze/iowa\_sales/}: Datos crudos en formato Parquet (32,816,143 registros)
\end{itemize}

\paragraph{Archivos Parquet en capa Bronze.}
BigQuery generó múltiples archivos Parquet para optimizar la exportación paralela. Cada archivo contiene un subconjunto de los registros totales:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/cloud-storage-images/gcs_bronze_layer_parquet_files.png}
  \caption{Archivos Parquet en la capa Bronze listos para procesamiento distribuido.}
  \label{fig:gcs-bronze}
\end{figure}

\paragraph{Validaciones realizadas.}
\begin{itemize}
  \item \textbf{Conteo de registros:} 32,816,143 registros exportados (coincide con el dataset original)
  \item \textbf{Formato:} Parquet columnar con compresión Snappy
  \item \textbf{Esquema:} Todas las columnas originales + columnas derivadas \texttt{year} y \texttt{month}
  \item \textbf{Integridad:} Sin errores de exportación, todos los archivos accesibles
  \item \textbf{Trazabilidad:} Métricas de tiempo registradas en \texttt{job\_timing\_bronze.json}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Procesamiento distribuido en Dataproc}

\subsubsection{Configuración de clusters}

Se provisionaron dos configuraciones de clusters Dataproc mediante \textbf{infraestructura automatizada como código (Terraform)}, permitiendo despliegues reproducibles y parametrizables para evaluar el impacto del tamaño y tipo de máquina en el rendimiento del procesamiento distribuido:

\paragraph{Cluster 1: Configuración estándar (n1-standard).}
Cluster con perfil balanceado de CPU y memoria, optimizado para cargas de trabajo generales:

\begin{table}[H]
  \centering
  \caption{Configuración del Cluster 1 (n1-standard-3w).}
  \label{tab:cluster1-config}
  \scriptsize
  \begin{tabular}{@{}lll@{}}
    \toprule
    Componente & Tipo de Máquina & Recursos \\
    \midrule
    Master & n1-standard-2 & 2 vCPUs, 7.5 GB RAM \\
    Workers (3x) & n1-standard-2 & 2 vCPUs, 7.5 GB RAM (cada uno) \\
    \midrule
    \textbf{Total} & & \textbf{8 vCPUs, 30 GB RAM} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/standard-cluster-images/cluster1_vm_instances_configuration.png}
  \caption{Configuración de instancias VM del Cluster 1 en Dataproc.}
  \label{fig:cluster1-vms}
\end{figure}

\paragraph{Cluster 2: Configuración high-memory (n2-highmem).}
Cluster con perfil de alta memoria, optimizado para cargas de trabajo intensivas en memoria:

\begin{table}[H]
  \centering
  \caption{Configuración del Cluster 2 (n2-highmem-4w).}
  \label{tab:cluster2-config}
  \scriptsize
  \begin{tabular}{@{}lll@{}}
    \toprule
    Componente & Tipo de Máquina & Recursos \\
    \midrule
    Master & n2-highmem-4 & 4 vCPUs, 32 GB RAM \\
    Workers (4x) & n2-highmem-2 & 2 vCPUs, 16 GB RAM (cada uno) \\
    \midrule
    \textbf{Total} & & \textbf{12 vCPUs, 96 GB RAM} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/high-mem-images/cluster2_vm_instances_configuration.png}
  \caption{Configuración de instancias VM del Cluster 2 en Dataproc.}
  \label{fig:cluster2-vms}
\end{figure}

\paragraph{Infraestructura como código (Terraform).}
Los clusters se provisionan mediante módulos de Terraform con variables parametrizables:

\begin{verbatim}
# cluster1.tfvars
cluster_name        = "iowa-cluster-n1-std-3w"
master_machine_type = "n1-standard-2"
worker_machine_type = "n1-standard-2"
num_workers         = 3

# cluster2.tfvars
cluster_name        = "iowa-cluster-n2-hm-4w"
master_machine_type = "n2-highmem-4"
worker_machine_type = "n2-highmem-2"
num_workers         = 4
\end{verbatim}

\subsubsection{Lectura del dataset desde GCS mediante PySpark}

El script \texttt{gold\_transform.py} inicializa una sesión de Spark y lee los datos de la capa Bronze almacenados en formato Parquet:

\begin{verbatim}
BRONZE_PATH = f"gs://{BUCKET}/bronze/iowa_sales"
df = spark.read.parquet(BRONZE_PATH)
records_read = df.count()  # 32,816,143 registros
\end{verbatim}

PySpark distribuye automáticamente la lectura de los múltiples archivos Parquet entre los workers del cluster, aprovechando el paralelismo para optimizar el tiempo de carga.

\subsubsection{Aplicación de limpieza, filtrado y transformación}

El procesamiento de la capa Gold se divide en tres etapas principales:

\paragraph{1. Limpieza de datos.}
Se aplican filtros para eliminar registros con valores nulos o inconsistentes en campos críticos:

\begin{verbatim}
df_clean = df.filter(
    (F.col("sale_dollars").isNotNull()) & (F.col("sale_dollars") > 0)
    & (F.col("bottles_sold").isNotNull()) & (F.col("bottles_sold") > 0)
    & (F.col("volume_sold_liters").isNotNull()) 
    & (F.col("volume_sold_liters") > 0)
).dropDuplicates()
\end{verbatim}

\textbf{Resultado:} De 32,816,143 registros iniciales, se limpiaron 32,801,412 registros (99.96\% de retención), eliminando solo 14,731 registros (0.04\%) con datos inconsistentes.

\paragraph{2. Feature engineering.}
Se generan características derivadas para enriquecer el dataset y mejorar el potencial predictivo:

\begin{itemize}
  \item \texttt{day\_of\_week}: Día de la semana (1-7) extraído de la fecha
  \item \texttt{quarter}: Trimestre del año (1-4)
  \item \texttt{is\_weekend}: Indicador binario (1 si es fin de semana, 0 si no)
  \item \texttt{price\_per\_bottle}: Precio unitario calculado como \texttt{sale\_dollars / bottles\_sold}
  \item \texttt{volume\_per\_bottle}: Volumen unitario calculado como \texttt{volume\_sold\_liters / bottles\_sold}
\end{itemize}

\begin{verbatim}
df_features = (
    df_clean
    .withColumn("day_of_week", F.dayofweek("date"))
    .withColumn("quarter", F.quarter("date"))
    .withColumn("is_weekend", 
        F.when(F.dayofweek("date").isin([1, 7]), 1).otherwise(0))
    .withColumn("price_per_bottle", 
        F.col("sale_dollars") / F.col("bottles_sold"))
    .withColumn("volume_per_bottle", 
        F.col("volume_sold_liters") / F.col("bottles_sold"))
)
\end{verbatim}

\paragraph{3. Escritura particionada a capa Gold.}
Los datos transformados se escriben en formato Parquet con particionamiento por año y mes para optimizar consultas futuras:

\begin{verbatim}
GOLD_PATH = f"gs://{BUCKET}/gold_{CLUSTER_NAME}/iowa_sales"
df_features.write.mode("overwrite")
    .partitionBy("year", "month")
    .parquet(GOLD_PATH)
\end{verbatim}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/cloud-storage-images/gcs_gold_cluster2_year_partitions.png}
  \caption{Estructura particionada por año en la capa Gold (Cluster 2).}
  \label{fig:gold-partitions}
\end{figure}

\subsubsection{Monitoreo de ejecución}

Durante la ejecución de los jobs, se monitorearon métricas de recursos y tiempos mediante la interfaz de Spark History Server y YARN:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/standard-cluster-images/cluster1_history_server_job_summary.png}
  \caption{Resumen del job en Spark History Server (Cluster 1).}
  \label{fig:cluster1-history}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/high-mem-images/cluster2_history_server_job_summary.png}
  \caption{Resumen del job en Spark History Server (Cluster 2).}
  \label{fig:cluster2-history}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/standard-cluster-images/cluster1_monitoring_cpu_memory.png}
  \caption{Monitoreo de CPU y memoria durante ejecución (Cluster 1).}
  \label{fig:cluster1-monitoring}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/high-mem-images/cluster2_monitoring_cpu_memory.png}
  \caption{Monitoreo de CPU y memoria durante ejecución (Cluster 2).}
  \label{fig:cluster2-monitoring}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelado predictivo en PySpark (***PLANNED***)}

\subsubsection{Modelo seleccionado}
[Descripción del modelo de regresión lineal seleccionado]

\subsubsection{Entrenamiento del modelo}
[Proceso de entrenamiento sobre Gold layer]

\subsubsection{Métricas de evaluación}
[Tabla con métricas: R², RMSE, MAE]

\begin{table}[H]
  \centering
  \caption{Métricas de evaluación del modelo.}
  \label{tab:model-metrics}
  \scriptsize
  \begin{tabular}{@{}lcc@{}}
    \toprule
    Métrica & Cluster 1 & Cluster 2 \\
    \midrule
    R² & [valor] & [valor] \\
    RMSE & [valor] & [valor] \\
    MAE & [valor] & [valor] \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluación comparativa entre configuraciones de cluster}

\subsubsection{Métricas de tiempo de ejecución}

\begin{table}[H]
  \centering
  \caption{Comparativa de tiempos de ejecución.}
  \label{tab:time-comparison}
  \scriptsize
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Etapa & Cluster 1 & Cluster 2 & Diferencia \\
    \midrule
    Lectura Bronze & [tiempo] & [tiempo] & [\%] \\
    Transformación & [tiempo] & [tiempo] & [\%] \\
    Escritura Gold & [tiempo] & [tiempo] & [\%] \\
    Total & [tiempo] & [tiempo] & [\%] \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
%    \includegraphics[width=0.7\linewidth]{figures/job-ui-metrics.png}
  \caption{Job UI de Dataproc mostrando métricas de tiempo y recursos.}
  \label{fig:job-ui}
\end{figure}

\subsubsection{Análisis de latencia, paralelismo y escalabilidad}
[Interpretación de cómo el tamaño del cluster afecta la ejecución]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4. RESULTADOS Y ANÁLISIS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Métricas, gráficas y análisis de resultados}

\subsection{Interpretación de resultados}
[Análisis de los resultados obtenidos]

\subsection{Justificación del muestreo}

En este proyecto se tomó la decisión de \textbf{no aplicar muestreo} y trabajar con el \textbf{dataset completo} de 32,816,143 registros. Esta decisión se fundamenta en principios de MLOps y mejores prácticas de ciencia de datos a gran escala:

\paragraph{Razones técnicas y de negocio:}

\begin{enumerate}
  \item \textbf{Capacidad de infraestructura distribuida:} El uso de Dataproc con PySpark permite procesar eficientemente el volumen completo de datos mediante paralelización. El muestreo habría subutilizado la capacidad del cluster y no habría justificado el costo de la infraestructura distribuida.
  
  \item \textbf{Representatividad y generalización:} Al utilizar el dataset completo, se garantiza que el modelo capture todos los patrones, estacionalidades y variaciones presentes en 13.8 años de datos históricos. Un muestreo podría introducir sesgos al excluir eventos raros pero importantes (e.g., ventas atípicas, productos de baja frecuencia, tiendas pequeñas).
  
  \item \textbf{Cobertura de alta cardinalidad:} Con 15,183 productos únicos, 3,337 tiendas y 504 ciudades, un muestreo podría excluir combinaciones importantes de características que son críticas para la predicción en segmentos específicos del negocio.
  
  \item \textbf{Validación de escalabilidad:} Uno de los objetivos del proyecto es demostrar la capacidad de procesamiento distribuido en entornos de datos masivos. Trabajar con el dataset completo valida que el pipeline puede manejar volúmenes reales de producción sin degradación de performance.
  
  \item \textbf{Reproducibilidad y trazabilidad:} Al no aplicar muestreo aleatorio, se elimina una fuente de variabilidad en los resultados. Cada ejecución del pipeline procesa exactamente los mismos datos, facilitando la reproducibilidad y el debugging.
  
  \item \textbf{Optimización de costos:} Aunque procesar el dataset completo consume más recursos computacionales, el tiempo total de ejecución (17.93 minutos en Cluster 2) es aceptable y el costo incremental es marginal comparado con el valor de tener un modelo entrenado sobre datos completos.
\end{enumerate}

\paragraph{Estrategia de validación sin muestreo:}

En lugar de muestreo para reducir volumen, se implementaron las siguientes estrategias:

\begin{itemize}
  \item \textbf{Particionamiento temporal:} Los datos se particionan por año y mes, permitiendo procesamiento incremental y consultas eficientes sobre ventanas temporales específicas.
  
  \item \textbf{Limpieza selectiva:} Se eliminaron únicamente registros con valores nulos o inconsistentes (0.04\% del total), preservando el 99.96\% de los datos válidos.
  
  \item \textbf{Feature engineering distribuido:} Las transformaciones se ejecutan en paralelo sobre el dataset completo, aprovechando la arquitectura distribuida de Spark.
  
  \item \textbf{Evaluación comparativa de clusters:} Se validó que ambas configuraciones de cluster pueden procesar el volumen completo, con el Cluster 2 (high-memory) completando en 17.93 minutos vs 28.44 minutos del Cluster 1.
\end{itemize}

\subsection{Evaluación del desempeño del modelo}
[Análisis crítico del desempeño]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 5. ANÁLISIS CRÍTICO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Análisis crítico del enfoque}

\subsection{Ventajas del enfoque elegido}
\begin{itemize}
  \item [Ventaja 1]
  \item [Ventaja 2]
  \item [Ventaja 3]
\end{itemize}

\subsection{Limitaciones del enfoque elegido}
\begin{itemize}
  \item [Limitación 1]
  \item [Limitación 2]
  \item [Limitación 3]
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 6. CONCLUSIONES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusiones}

[Conclusiones generales del proyecto]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 7. CÓDIGO Y REPOSITORIO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Código utilizado}

\subsection{Script principal de PySpark}
[Referencia al script principal]

\subsection{Repositorio de código fuente}
\url{https://github.com/[usuario]/ML-BigData}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCIAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Referencias}

\begin{itemize}
  \item Google LLC (s. f.). Google Cloud Console. \url{https://console.cloud.google.com/}
  \item Google Cloud. (2024). Crea un clúster de Dataproc con la consola de Google Cloud. \url{https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-console?hl=es-419}
  \item BigQuery Public Data. Iowa Liquor Sales. \url{https://console.cloud.google.com/marketplace/product/iowa-department-of-commerce/iowa-liquor-sales}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
