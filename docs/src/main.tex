\documentclass[10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\decimalpoint
\usepackage{amsmath, amssymb}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{letterpaper, margin=1in}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{tocloft}

\usepackage[colorlinks=true, linkcolor=black, urlcolor=black, citecolor=black]{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT METADATA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Universidad Panamericana \\ 
    Maestría en Ciencia de Datos \\ 
    Datos Masivos \\ 
    \vspace{0.5cm} 
    Proyecto Final: \textit{Pipeline Distribuido de Predicción para Iowa Liquor Sales en GCP}}
\author{Enrique Ulises Báez Gómez Tagle, Luis Alejandro Guillén Alvarez}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. DATASET UTILIZADO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset utilizado}
\subsection{Fuente y descripción}

El dataset utilizado proviene de \textbf{BigQuery Public Data} y contiene registros de ventas de licores en el estado de Iowa, Estados Unidos. Este conjunto de datos es mantenido por el Iowa Department of Commerce y está disponible públicamente para análisis.

\begin{itemize}
  \item \textbf{Fuente:} BigQuery Public Data - \texttt{bigquery-public-data.iowa\_liquor\_sales.sales}
  \item \textbf{Tamaño:} 32,816,143 registros
  \item \textbf{Periodo:} 2012-01-03 a 2025-10-31 (13.8 años)
  \item \textbf{Características principales:}
  \begin{itemize}
    \item \texttt{date}: Fecha de la transacción
    \item \texttt{store\_number}: Identificador de la tienda
    \item \texttt{city}: Ciudad donde se realizó la venta
    \item \texttt{category}: Categoría del producto
    \item \texttt{item\_number}: Identificador del producto
    \item \texttt{sale\_dollars}: Monto de la venta (variable objetivo)
    \item \texttt{bottles\_sold}: Cantidad de botellas vendidas
    \item \texttt{volume\_sold\_liters}: Volumen vendido en litros
  \end{itemize}
\end{itemize}

\subsection{Cardinalidades y dimensiones}

El dataset presenta alta cardinalidad en múltiples dimensiones, lo que lo hace ideal para procesamiento distribuido:

\begin{table}[H]
  \centering
  \caption{Cardinalidades del dataset Iowa Liquor Sales.}
  \label{tab:cardinalities}
  \scriptsize
  \begin{tabular}{@{}lr@{}}
    \toprule
    Dimensión & Valores Únicos \\
    \midrule
    Tiendas & 3,337 \\
    Ciudades & 504 \\
    Productos & 15,183 \\
    Categorías & 185 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Calidad de los datos}

El análisis exploratorio reveló una excelente calidad de datos con mínimos valores faltantes:

\begin{table}[H]
  \centering
  \caption{Valores nulos por campo.}
  \label{tab:missing-values}
  \scriptsize
  \begin{tabular}{@{}lrr@{}}
    \toprule
    Campo & Valores Nulos & Porcentaje \\
    \midrule
    sale\_dollars & 10 & 0.00003\% \\
    category & 16,974 & 0.052\% \\
    city & 84,575 & 0.258\% \\
    \midrule
    \textbf{Total} & \textbf{101,559} & \textbf{0.31\%} \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Calidad general:} 99.69\% de datos completos, lo que indica un dataset de alta calidad para modelado predictivo.

\subsection{Distribución de ventas}

La distribución de la variable objetivo (\texttt{sale\_dollars}) muestra las siguientes características:

\begin{table}[H]
  \centering
  \caption{Distribución de ventas en dólares.}
  \label{tab:sales-distribution}
  \scriptsize
  \begin{tabular}{@{}lr@{}}
    \toprule
    Percentil & Valor (USD) \\
    \midrule
    P50 (Mediana) & \$78.66 \\
    P90 & \$269.88 \\
    P99 & \$1,185.60 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Top 10 categorías por volumen de ventas}

Las categorías más vendidas representan una parte significativa del volumen total de transacciones:

\begin{table}[H]
  \centering
  \caption{Top 10 categorías por ventas totales.}
  \label{tab:top-categories}
  \scriptsize
  \begin{tabular}{@{}rlrr@{}}
    \toprule
    Rank & Categoría & Ventas Totales (USD) & Transacciones \\
    \midrule
    1 & 1012100.0 & \$495,078,200 & 2,778,490 \\
    2 & 1031100.0 & \$441,329,100 & 2,988,622 \\
    3 & 1011200.0 & \$288,427,900 & 1,859,256 \\
    4 & 1081600.0 & \$219,643,200 & 1,360,017 \\
    5 & 1062400.0 & \$169,326,700 & 861,360 \\
    6 & 1022200.0 & \$152,794,300 & 668,286 \\
    7 & 1031080.0 & \$145,760,500 & 1,265,930 \\
    8 & 1022100.0 & \$143,383,100 & 849,580 \\
    9 & 1011400.0 & \$119,534,300 & 538,956 \\
    10 & 1011100.0 & \$117,536,600 & 1,213,606 \\
    \midrule
    \textbf{Total Top 10} & & \textbf{\$2,292,813,900} & \textbf{15,384,103} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Justificación de selección}

Este dataset fue seleccionado por las siguientes razones:

\begin{enumerate}
  \item \textbf{Volumen masivo:} Con más de 32 millones de registros, cumple ampliamente con el requisito de ≥32M registros y justifica el uso de procesamiento distribuido con PySpark en Dataproc.
  
  \item \textbf{Datos temporales:} El rango de 13.8 años permite análisis de series temporales y patrones estacionales, ideal para feature engineering temporal.
  
  \item \textbf{Alta dimensionalidad:} La combinación de 15K+ productos, 185 categorías, 3.3K tiendas y 504 ciudades proporciona un espacio de características rico para modelado predictivo.
  
  \item \textbf{Calidad excepcional:} Con 99.69\% de datos completos, minimiza la necesidad de imputación compleja y permite enfocarse en transformaciones y modelado.
  
  \item \textbf{Variable objetivo continua:} \texttt{sale\_dollars} es una variable continua ideal para regresión lineal, permitiendo predecir montos de venta basados en características de productos, ubicación y temporalidad.
  
  \item \textbf{Disponibilidad pública:} Al estar en BigQuery Public Data, facilita la reproducibilidad del proyecto y el acceso sin restricciones de licenciamiento.
  
  \item \textbf{Relevancia práctica:} Los modelos predictivos de ventas tienen aplicaciones directas en optimización de inventario, planificación de demanda y estrategias de pricing.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. ARQUITECTURA IMPLEMENTADA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Descripción de la arquitectura implementada}

\subsection{Diagrama de arquitectura}

La arquitectura implementada sigue un patrón de medallion con dos capas (Bronze y Gold) sobre Google Cloud Platform, integrando servicios de almacenamiento, procesamiento distribuido y análisis de datos masivos.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../figures/architecture/iowa-liquor-sales-ml-pipeline-architecture.png}
    \caption{Arquitectura del pipeline distribuido: BigQuery → Cloud Run → GCS Bronze → Dataproc → GCS Gold → ML Model (***PLANNED***).}
    \label{fig:architecture}
\end{figure}

\subsection{Flujo de datos}

El pipeline implementa un flujo de datos end-to-end con las siguientes etapas:

\begin{enumerate}
  \item \textbf{Fuente de datos (BigQuery):} El dataset público \texttt{iowa\_liquor\_sales} (32M+ registros) sirve como origen de datos. Los scripts de EDA (\texttt{eda\_iowa.py} y \texttt{eda\_iowa.ipynb}) realizan análisis exploratorio inicial directamente sobre BigQuery.
  
  \item \textbf{Extracción (Cloud Run):} Un servicio ETL desplegado en Cloud Run ejecuta \texttt{bronze\_extract.py}, que extrae datos desde BigQuery y los carga en formato Parquet particionado hacia la capa Bronze en Google Cloud Storage.
  
  \item \textbf{Capa Bronze (GCS):} Almacenamiento de datos crudos en formato Parquet con particionamiento temporal, preservando la estructura original para trazabilidad y reproducibilidad.
  
  \item \textbf{Transformación (Dataproc):} Un cluster de Dataproc ejecuta \texttt{gold\_transform.py} con PySpark, aplicando limpieza, transformaciones y feature engineering sobre los datos Bronze. El procesamiento distribuido permite manejar el volumen masivo de forma eficiente.
  
  \item \textbf{Capa Gold (GCS):} Datos limpios, transformados y enriquecidos con features derivadas, almacenados en formato Parquet particionado y optimizados para consumo analítico y modelado ML.
  
  \item \textbf{Modelado ML (***PLANNED***):} Modelo de regresión lineal con PySpark MLlib entrenado sobre la capa Gold para predicción de ventas, con evaluación de métricas (R², RMSE, MAE) y comparación de performance entre configuraciones de cluster.
\end{enumerate}

\subsection{Componentes de la arquitectura}

\begin{itemize}
  \item \textbf{BigQuery:} Fuente de datos pública (\texttt{bigquery-public-data.iowa\_liquor\_sales.sales})
  \item \textbf{Cloud Run:} Servicio ETL serverless para extracción batch hacia capa Bronze
  \item \textbf{GCS Bronze Layer:} Almacenamiento de datos crudos en formato Parquet particionado
  \item \textbf{Dataproc (PySpark):} Cluster de procesamiento distribuido para transformación y feature engineering
  \item \textbf{GCS Gold Layer:} Datos refinados listos para análisis y modelado
  \item \textbf{Terraform:} Infraestructura como código para provisionar clusters Dataproc con diferentes configuraciones
  \item \textbf{ML Model (***PLANNED***):} Modelo de regresión PySpark MLlib para predicción de ventas
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3. DESARROLLO DE LA RUTA ELEGIDA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Desarrollo de la ruta elegida: Procesamiento Distribuido con PySpark}

\subsection{Selección y exportación del dataset hacia GCS}

\subsubsection{Proceso de exportación}

La extracción de datos desde BigQuery hacia Google Cloud Storage se implementó mediante un servicio ETL desplegado en Cloud Run, diseñado para ejecutarse como job batch serverless. El proceso consta de tres etapas principales:

\paragraph{Etapa 1: Creación de tabla temporal con particionamiento.}
El script \texttt{bronze\_extract.py} ejecuta una consulta SQL que selecciona todos los registros del dataset público y agrega columnas derivadas de año y mes para facilitar el particionamiento posterior en PySpark:

\begin{verbatim}
SELECT 
    *,
    EXTRACT(YEAR FROM date) as year,
    EXTRACT(MONTH FROM date) as month
FROM `bigquery-public-data.iowa_liquor_sales.sales`
\end{verbatim}

Esta consulta materializa una tabla temporal en el dataset \texttt{ml\_work.bronze\_temp} del proyecto, permitiendo una exportación eficiente sin modificar la fuente original.

\paragraph{Etapa 2: Exportación a formato Parquet.}
Utilizando la API de BigQuery, se exportan los datos desde la tabla temporal hacia Google Cloud Storage en formato Parquet, un formato columnar optimizado para procesamiento distribuido:

\begin{itemize}
  \item \textbf{Destino:} \texttt{gs://iowa-liquor-medallion-ml/bronze/iowa\_sales/*.parquet}
  \item \textbf{Formato:} Parquet (columnar, comprimido)
  \item \textbf{Particionamiento:} Múltiples archivos generados automáticamente por BigQuery
\end{itemize}

\paragraph{Etapa 3: Limpieza y registro de métricas.}
Una vez completada la exportación, se elimina la tabla temporal y se registran las métricas de tiempo en un archivo JSON almacenado en GCS para trazabilidad.

\subsubsection{Configuración de Cloud Run}

El servicio se despliega mediante un contenedor Docker con las siguientes características:

\begin{itemize}
  \item \textbf{Imagen base:} \texttt{python:3.11-slim}
  \item \textbf{Dependencias:} \texttt{google-cloud-bigquery}, \texttt{google-cloud-storage}, \texttt{pandas}, \texttt{pyarrow}, \texttt{db-dtypes}
  \item \textbf{Tipo de ejecución:} Cloud Run Job (batch, no HTTP)
  \item \textbf{Variables de entorno:}
  \begin{itemize}
    \item \texttt{PROJECT\_ID}: \texttt{secure-cipher-475203-k2}
    \item \texttt{BUCKET\_NAME}: \texttt{iowa-liquor-medallion-ml}
  \end{itemize}
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/cloud-run-images/cloudrun_job_execution_history_config.png}
  \caption{Configuración y historial de ejecución del Cloud Run Job para extracción Bronze.}
  \label{fig:cloudrun-config}
\end{figure}

\subsubsection{Métricas de tiempo de ejecución}

La fase de extracción Bronze completó exitosamente con las siguientes métricas:

\begin{table}[H]
  \centering
  \caption{Tiempos de ejecución de la fase Bronze (Cloud Run).}
  \label{tab:bronze-timing}
  \scriptsize
  \begin{tabular}{@{}lr@{}}
    \toprule
    Etapa & Tiempo \\
    \midrule
    Creación de tabla temporal & 5.51s \\
    Exportación a Parquet (GCS) & 2.81s \\
    Limpieza de recursos & 0.16s \\
    \midrule
    \textbf{Tiempo total} & \textbf{8.51s (0.14 min)} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/cloud-run-images/cloudrun_observability_metrics.png}
  \caption{Métricas de observabilidad del Cloud Run Job mostrando ejecución exitosa.}
  \label{fig:cloudrun-metrics}
\end{figure}

\subsubsection{Verificación de estructura y consistencia}

Una vez completada la exportación, se verificó la estructura del bucket de GCS y la integridad de los datos:

\paragraph{Estructura del bucket.}
El bucket \texttt{iowa-liquor-medallion-ml} queda entonces con la siguiente carpeta:

\begin{itemize}
  \item \texttt{bronze/iowa\_sales/}: Datos crudos en formato Parquet (32,816,143 registros)
\end{itemize}

\paragraph{Archivos Parquet en capa Bronze.}
BigQuery generó múltiples archivos Parquet para optimizar la exportación paralela. Cada archivo contiene un subconjunto de los registros totales:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{../img/cloud-storage-images/gcs_bronze_layer_parquet_files.png}
  \caption{Archivos Parquet en la capa Bronze listos para procesamiento distribuido.}
  \label{fig:gcs-bronze}
\end{figure}

\paragraph{Validaciones realizadas.}
\begin{itemize}
  \item \textbf{Conteo de registros:} 32,816,143 registros exportados (coincide con el dataset original)
  \item \textbf{Formato:} Parquet columnar con compresión Snappy
  \item \textbf{Esquema:} Todas las columnas originales + columnas derivadas \texttt{year} y \texttt{month}
  \item \textbf{Integridad:} Sin errores de exportación, todos los archivos accesibles
  \item \textbf{Trazabilidad:} Métricas de tiempo registradas en \texttt{job\_timing\_bronze.json}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Procesamiento distribuido en Dataproc}

\subsubsection{Configuración del cluster}

\begin{table}[H]
  \centering
  \caption{Configuración de clusters Dataproc.}
  \label{tab:cluster-config}
  \scriptsize
  \begin{tabular}{@{}llllll@{}}
    \toprule
    Cluster & Tipo Nodo & Cantidad & vCPU & Memoria & Disco \\
    \midrule
    Cluster 1 & [tipo] & [n] & [vCPU] & [RAM] & [GB] \\
    Cluster 2 & [tipo] & [n] & [vCPU] & [RAM] & [GB] \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Lectura del dataset desde GCS}
[Código y descripción de lectura con PySpark]

\subsubsection{Limpieza, filtrado y transformación}
[Descripción de las transformaciones aplicadas]

\begin{itemize}
  \item Limpieza de valores nulos
  \item Filtrado de registros inconsistentes
  \item Transformación de tipos de datos
  \item Feature engineering
\end{itemize}

\begin{figure}[H]
  \centering
%    \includegraphics[width=0.7\linewidth]{figures/dataproc-cluster.png}
  \caption{Cluster de Dataproc ejecutando jobs de transformación.}
  \label{fig:dataproc-cluster}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelado predictivo en PySpark (***PLANNED***)}

\subsubsection{Modelo seleccionado}
[Descripción del modelo de regresión lineal seleccionado]

\subsubsection{Entrenamiento del modelo}
[Proceso de entrenamiento sobre Gold layer]

\subsubsection{Métricas de evaluación}
[Tabla con métricas: R², RMSE, MAE]

\begin{table}[H]
  \centering
  \caption{Métricas de evaluación del modelo.}
  \label{tab:model-metrics}
  \scriptsize
  \begin{tabular}{@{}lcc@{}}
    \toprule
    Métrica & Cluster 1 & Cluster 2 \\
    \midrule
    R² & [valor] & [valor] \\
    RMSE & [valor] & [valor] \\
    MAE & [valor] & [valor] \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluación comparativa entre configuraciones de cluster}

\subsubsection{Métricas de tiempo de ejecución}

\begin{table}[H]
  \centering
  \caption{Comparativa de tiempos de ejecución.}
  \label{tab:time-comparison}
  \scriptsize
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Etapa & Cluster 1 & Cluster 2 & Diferencia \\
    \midrule
    Lectura Bronze & [tiempo] & [tiempo] & [\%] \\
    Transformación & [tiempo] & [tiempo] & [\%] \\
    Escritura Gold & [tiempo] & [tiempo] & [\%] \\
    Total & [tiempo] & [tiempo] & [\%] \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
%    \includegraphics[width=0.7\linewidth]{figures/job-ui-metrics.png}
  \caption{Job UI de Dataproc mostrando métricas de tiempo y recursos.}
  \label{fig:job-ui}
\end{figure}

\subsubsection{Análisis de latencia, paralelismo y escalabilidad}
[Interpretación de cómo el tamaño del cluster afecta la ejecución]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4. RESULTADOS Y ANÁLISIS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Métricas, gráficas y análisis de resultados}

\subsection{Interpretación de resultados}
[Análisis de los resultados obtenidos]

\subsection{Justificación del muestreo}
[Explicación de las decisiones de muestreo si aplica]

\subsection{Evaluación del desempeño del modelo}
[Análisis crítico del desempeño]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 5. ANÁLISIS CRÍTICO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Análisis crítico del enfoque}

\subsection{Ventajas del enfoque elegido}
\begin{itemize}
  \item [Ventaja 1]
  \item [Ventaja 2]
  \item [Ventaja 3]
\end{itemize}

\subsection{Limitaciones del enfoque elegido}
\begin{itemize}
  \item [Limitación 1]
  \item [Limitación 2]
  \item [Limitación 3]
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 6. CONCLUSIONES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusiones}

[Conclusiones generales del proyecto]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 7. CÓDIGO Y REPOSITORIO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Código utilizado}

\subsection{Script principal de PySpark}
[Referencia al script principal]

\subsection{Repositorio de código fuente}
\url{https://github.com/[usuario]/ML-BigData}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCIAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Referencias}

\begin{itemize}
  \item Google LLC (s. f.). Google Cloud Console. \url{https://console.cloud.google.com/}
  \item Google Cloud. (2024). Crea un clúster de Dataproc con la consola de Google Cloud. \url{https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-console?hl=es-419}
  \item BigQuery Public Data. Iowa Liquor Sales. \url{https://console.cloud.google.com/marketplace/product/iowa-department-of-commerce/iowa-liquor-sales}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
